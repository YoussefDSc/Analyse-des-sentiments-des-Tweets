{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Dans ce projet, nous construirons **Logistic Regression Classifier** de polarité à trois sens (positif, négatif, neutre) pour les tweets, sans utiliser le moteur d'analyse de sentiment intégré à NLTK.\n",
    "\n",
    "Nous utiliserons **Logistic Regression Classifier**, des fonctions **bag-of-words** et des lexiques de polarité (à la fois intégrés et externes). Nous créerons également notre propre module de prétraitement pour traiter les tweets bruts.\n",
    "\n",
    "# Données utilisées\n",
    "- training.json : Ce fichier contient ~15k tweets bruts, avec leurs étiquettes de polarité (1 = positif, 0 = neutre, -1 = négatif). Nous utiliserons ce fichier pour former nos classificateurs.\n",
    "\n",
    "- develop.json : Dans le même format que training.json, le fichier contient un plus petit ensemble de tweets. Nous l'utiliserons pour tester les prédictions de nos classificateurs qui ont été entraînés sur le plateau d'entraînement.\n",
    "\n",
    "# Prétraitement\n",
    "La première chose que nous allons faire est de prétraiter les tweets pour qu'ils soient plus faciles à gérer, et prêts pour l'extraction de fonctionnalités, et la formation par les classificateurs.\n",
    "\n",
    "Pour commencer, nous allons extraire les tweets du fichier json, lire chaque ligne et stocker les tweets, les étiquettes dans des listes séparées.\n",
    "\n",
    "Ensuite, pour le prétraitement, nous allons :\n",
    "\n",
    "- segmenter les tweets en phrases à l'aide d'un segmenteur NTLK\n",
    "- extraire les phrases à l'aide d'un tokenizer NLTK\n",
    "- tous les mots en minuscules\n",
    "- supprimer les noms d'utilisateur twitter commençant par @ en utilisant regex\n",
    "- supprimer les URL commençant par http en utilisant regex \n",
    "- nous allons extraire les hashtags, et essayer de les décomposer en multi-mots en utilisant un algorithme MaxMatch, et le dictionnaire de mots anglais fourni avec NLTK.\n",
    "\n",
    "Construisons quelques fonctions pour accomplir tout cela.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "dictionary = set(nltk.corpus.words.words()) #To be used for MaxMatch\n",
    "\n",
    "#Fonction pour lemmatiser le mot | Utilisé pendant maxmatch\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "#Fonction pour implémenter l'algorithme maxmatch pour les hashtags multi-mots\n",
    "def maxmatch(word,dictionary):\n",
    "    if not word:\n",
    "        return []\n",
    "    for i in range(len(word),1,-1):\n",
    "        first = word[0:i]\n",
    "        rem = word[i:]\n",
    "        if lemmatize(first).lower() in dictionary: #Il est important de lemmatiser les mots en minuscules avant de les comparer dans le dictionnaire. \n",
    "            return [first] + maxmatch(rem,dictionary)\n",
    "    first = word[0:1]\n",
    "    rem = word[1:]\n",
    "    return [first] + maxmatch(rem,dictionary)\n",
    "\n",
    "#Fonction pour prétraiter un seul tweet dans le dictionnaire.\n",
    "def preprocess(tweet):\n",
    "    \n",
    "    tweet = re.sub(\"@\\w+\",\"\",tweet).strip()\n",
    "    tweet = re.sub(\"http\\S+\",\"\",tweet).strip()\n",
    "    hashtags = re.findall(\"#\\w+\",tweet)\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"#\\w+\",\"\",tweet).strip() \n",
    "    \n",
    "    hashtag_tokens = [] #Liste séparée pour les hashtags\n",
    "    \n",
    "    for hashtag in hashtags:\n",
    "        hashtag_tokens.append(maxmatch(hashtag[1:],dictionary))        \n",
    "    \n",
    "    segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    segmented_sentences = segmenter.tokenize(tweet)\n",
    "    \n",
    "    #La symbolisation générale\n",
    "    processed_tweet = []\n",
    "    \n",
    "    word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "    for sentence in segmented_sentences:\n",
    "        tokenized_sentence = word_tokenizer.tokenize(sentence.strip())\n",
    "        processed_tweet.append(tokenized_sentence)\n",
    "    \n",
    "    #Traitement des hashtags seulement quand ils existent dans un tweet\n",
    "    if hashtag_tokens:\n",
    "        for tag_token in hashtag_tokens:\n",
    "            processed_tweet.append(tag_token)\n",
    "    \n",
    "    return processed_tweet\n",
    "    \n",
    "#Fonction personnalisée qui prend un fichier, et passe chaque tweet au préprocesseur\n",
    "def preprocess_file(filename):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        tweets.append(preprocess(tweet_dict[\"text\"]))\n",
    "        labels.append(int(tweet_dict[\"label\"]))\n",
    "    return tweets,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'exécuter le prétraitement de nos données d'entraînement, voyons comment fonctionne l'algorithme maxmatch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'can']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxmatch('wecan',dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de lui donner quelque chose de plus dur que ça.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cases', 'tu', 'd', 'y']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxmatch('casestudy',dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir dans l'exemple ci-dessus, il décompose incorrectement le mot'casestudy', en renvoyant'cases', au lieu de'case' pour la première itération, ce qui aurait été un meilleur résultat. C'est parce qu'il extrait avec gourmandise les'cases' en premier.\n",
    "\n",
    "Pour une amélioration, nous pouvons implémenter un algorithme qui compte mieux le nombre total de correspondances réussies dans le résultat du processus maxmatch, et retourner celui qui a le plus grand nombre de correspondances réussies.\n",
    "\n",
    "Exécutons notre module de prétraitement sur les données brutes de formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exécuter le module de prétraitement de base et capturer les données (peut-être passer au bloc suivant)\n",
    "train_data = preprocess_file(r'C:\\Users\\acer\\Desktop\\training.json')\n",
    "train_tweets = train_data[0]\n",
    "train_labels = train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['dear', 'the', 'newooffice', 'for', 'mac', 'is', 'great', 'and', 'all', ',', 'but', 'no', 'lync', 'update', '?'], ['c', \"'\", 'mon', '.']], [['how', 'about', 'you', 'make', 'a', 'system', 'that', 'doesn', \"'\", 't', 'eat', 'my', 'friggin', 'discs', '.'], ['this', 'is', 'the', '2nd', 'time', 'this', 'has', 'happened', 'and', 'i', 'am', 'so', 'sick', 'of', 'it', '!']]]\n"
     ]
    }
   ],
   "source": [
    "print (train_tweets[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... on peut faire mieux que ça pour comprendre ce qui se passe. Ecrivons un script simple qui lancera le module de prétraitement sur quelques tweets, et imprimera les résultats originaux et traités, côte à côte, s'il détecte un hashtag multi-mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tweet original : If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft\n",
      "tweet traité: [['if', 'i', 'make', 'a', 'game', 'as', 'a', 'universal', 'app', '.'], ['will', 'owners', 'be', 'able', 'to', 'download', 'and', 'play', 'it', 'in', 'november', '?'], ['windows', '1', '0'], ['x', 'box', 'one']]\n",
      "\n",
      "2. Tweet original : Microsoft, I may not prefer your gaming branch of business. But, you do make a damn fine operating system. #Windows10 @Microsoft\n",
      "tweet traité: [['microsoft', ',', 'i', 'may', 'not', 'prefer', 'your', 'gaming', 'branch', 'of', 'business', '.'], ['but', ',', 'you', 'do', 'make', 'a', 'damn', 'fine', 'operating', 'system', '.'], ['Window', 's', '1', '0']]\n",
      "\n",
      "3. Tweet original : @MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail\n",
      "tweet traité: [['i', 'will', 'be', 'downgrading', 'and', 'let', 'be', 'out', 'for', 'almost', 'the', '1st', 'yr', 'b4', 'trying', 'it', 'again', '.'], ['Window', 's', '1', '0'], ['Window', 's', '1', '0', 'fail']]\n",
      "\n",
      "4. Tweet original : @Microsoft 2nd computer with same error!!! #Windows10fail Guess we will shelve this until SP1! http://t.co/QCcHlKuy8Q\n",
      "tweet traité: [['2nd', 'computer', 'with', 'same', 'error', '!!!'], ['guess', 'we', 'will', 'shelve', 'this', 'until', 'sp1', '!'], ['Window', 's', '1', '0', 'fail']]\n",
      "\n",
      "5. Tweet original : Sunday morning, quiet day so time to welcome in #Windows10 @Microsoft @Windows http://t.co/7VtvAzhWmV\n",
      "tweet traité: [['sunday', 'morning', ',', 'quiet', 'day', 'so', 'time', 'to', 'welcome', 'in'], ['Window', 's', '1', '0']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing des exemples de hashtags multi-mots (ne fonctionne pas pour les tweets multiphrases)\n",
    "f = open(r'C:\\Users\\acer\\Desktop\\training.json')\n",
    "count = 1\n",
    "for index,line in enumerate(f):\n",
    "    if count >5:\n",
    "        break\n",
    "    original_tweet = json.loads(line)[\"text\"]\n",
    "    hashtags = re.findall(\"#\\w+\",original_tweet)\n",
    "    if hashtags:\n",
    "        for hashtag in hashtags:\n",
    "            if len(maxmatch(hashtag[1:],dictionary)) > 1:\n",
    "                #Si la longueur du tableau renvoyé par la fonction maxmatch est supérieure à 1,\n",
    "                #ca signifie que l'algorithme a détecté un hashtag avec plus d'un mot à l'intérieur \n",
    "                print (str(count) + \". Tweet original : \" + original_tweet + \"\\ntweet traité: \" + str(train_tweets[index]) + \"\\n\")\n",
    "                count += 1\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est mieux comme ça ! Notre module de prétraitement fonctionne comme prévu.\n",
    "\n",
    "L'étape suivante consiste à convertir chaque tweet traité en un dictionnaire de fonctionnalités pour un sac de mots. Nous allons permettre des options pour supprimer les mots vides pendant le processus, et aussi pour supprimer les mots rares, c'est-à-dire les mots qui apparaissent moins de n fois dans l'ensemble de la formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#Pour identifier les mots apparaissant moins de n fois, nous sommes en train de créer un dictionnaire pour l'ensemble de la formation.\n",
    "\n",
    "total_train_bow = {}\n",
    "\n",
    "for tweet in train_tweets:\n",
    "    for segment in tweet:\n",
    "        for token in segment:\n",
    "            total_train_bow[token] = total_train_bow.get(token,0) + 1\n",
    "\n",
    "#Fonction pour convertir les tweets pré_traités en dictionnaires de fonctionnalité de sac de mots\n",
    "#Permet de supprimer des options pour supprimer des mots vides, et aussi pour supprimer des mots apparaissant moins de n fois dans l'ensemble de l'entraînement.           \n",
    "def convert_to_feature_dicts(tweets,remove_stop_words,n): \n",
    "    feature_dicts = []\n",
    "    for tweet in tweets:\n",
    "        # Dictionnaire des fonctionnalités de build pour tweet\n",
    "        feature_dict = {}\n",
    "        if remove_stop_words:\n",
    "            for segment in tweet:\n",
    "                for token in segment:\n",
    "                    if token not in stopwords and (n<=0 or total_train_bow[token]>=n):\n",
    "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
    "        else:\n",
    "            for segment in tweet:\n",
    "                for token in segment:\n",
    "                    if n<=0 or total_train_bow[token]>=n:\n",
    "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
    "        feature_dicts.append(feature_dict)\n",
    "    return feature_dicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre fonction de conversion de tweets bruts en dictionnaires de fonctionnalités, nous allons l'exécuter sur training and development data. Nous convertirons également le dictionnaire des fonctionnalités en une représentation éparses, afin qu'il puisse être utilisé par les algorithmes ML de scikit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "#Conversion en dictionnaires de fonctionnalités\n",
    "train_set = convert_to_feature_dicts(train_tweets,True,2)\n",
    "\n",
    "dev_data = preprocess_file(r'C:\\Users\\acer\\Desktop\\training.json')\n",
    "\n",
    "dev_set = convert_to_feature_dicts(dev_data[0],False,0)\n",
    "\n",
    "#Conversion en représentations éparses\n",
    "training_data = vectorizer.fit_transform(train_set)\n",
    "\n",
    "development_data = vectorizer.transform(dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Maintenant, nous allons passer nos données dans decision tree classifier, et essayer d'ajuster les paramètres en utilisant la recherche par grille sur les combinaisons de paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramètres optimaux pour DT: {'criterion': 'entropy', 'max_features': None, 'min_samples_leaf': 75}\n",
      "\n",
      "Précision de la Decision Tree : 0.5729842308836656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Grille utilisée pour tester les combinaisons de paramètres\n",
    "tree_param_grid = [\n",
    "    {'criterion':['gini','entropy'], 'min_samples_leaf': [75,100,125,150,175], 'max_features':['sqrt','log2',None],\n",
    "    }\n",
    "]\n",
    "\n",
    "tree_clf = GridSearchCV(DecisionTreeClassifier(),tree_param_grid,cv=10,scoring='accuracy')\n",
    "\n",
    "tree_clf.fit(training_data,train_data[1])\n",
    "\n",
    "print (\"Paramètres optimaux pour DT: \" + str(tree_clf.best_params_)) #Pour print la meilleure combinaison découverte des paramètres\n",
    "\n",
    "tree_predictions = tree_clf.predict(development_data)\n",
    "\n",
    "print (\"\\nPrécision de la Decision Tree : \" + str(accuracy_score(dev_data[1],tree_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Le **decision tree classifier** ne semble pas fonctionner très bien, mais nous n'avons toujours pas de point de repère avec lequel le comparer.\n",
    "\n",
    "Exécutons nos données dans **dummy classifier** qui choisira la classe la plus fréquente comme sortie, à chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Précision de référence de la classe la plus courante: 0.42862243379946446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#Le classificateur fictif ci-dessous prédit toujours la classe la plus fréquente, comme spécifié dans la stratégie. \n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(development_data,dev_data[1])\n",
    "dummy_predictions = dummy_clf.predict(development_data)\n",
    "\n",
    "print (\"\\nPrécision de référence de la classe la plus courante: \" + str(accuracy_score(dev_data[1],dummy_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir que notre classificateur DT est au moins plus performant que le classificateur factice.\n",
    "\n",
    "Nous allons faire la même chose pour le classificateur **logisitc regression maintenant**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\acer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramètres optimaux pour LR: {'C': 0.012, 'multi_class': 'multinomial', 'solver': 'lbfgs'}\n",
      "Précision de la Logistic Regression  : 0.663373995834573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_param_grid = [\n",
    "    {'C':[0.012,0.0125,0.130,0.135,0.14],\n",
    "     'solver':['lbfgs'],'multi_class':['multinomial']\n",
    "    }\n",
    "]\n",
    "\n",
    "log_clf = GridSearchCV(LogisticRegression(),log_param_grid,cv=10,scoring='accuracy')\n",
    "\n",
    "log_clf.fit(training_data,train_data[1])\n",
    "\n",
    "log_predictions = log_clf.predict(development_data)\n",
    "\n",
    "print (\"Paramètres optimaux pour LR: \" + str(log_clf.best_params_))\n",
    "\n",
    "print (\"Précision de la Logistic Regression  : \" + str(accuracy_score(dev_data[1],log_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récapituler ce qui vient de se passer, nous avons créé un classificateur **logistic regression** en effectuant **grid search** des meilleurs paramètres pour C (paramètre de régularisation), le type de solveur et la gestion multi_class, tout comme nous l'avons fait pour **decision tree classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexiques de polarité\n",
    "Maintenant, nous allons essayer d'intégrer des informations externes dans l'ensemble de formation, sous la forme des scores de polarité pour les tweets.\n",
    "\n",
    "Nous allons construire deux lexiques automatiques, les comparer avec l'ensemble annoté manuellement de NLTK, puis ajouter cette information à nos données de formation.\n",
    "\n",
    "ce lexique sera construit via SentiWordNet. Ceci a pré-calculé des scores positifs, négatifs et neutres pour certains mots dans WordNet. Comme cette information est arrangée sous forme de synapses, nous prendrons simplement la polarité la plus commune à travers ses sens (et prendrons neutre en cas d'égalité)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doubtful', 'unclogged', 'niceness', 'Mama', 'ad_hominem']\n",
      "['unargumentative', 'ductless', 'dyspeptic', 'nonadhesive', 'countermand']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "\n",
    "swn_positive = []\n",
    "\n",
    "swn_negative = []\n",
    "\n",
    "#Fonction fournie avec l'affectation, non décrite ci-dessous.\n",
    "def get_polarity_type(synset_name):\n",
    "    swn_synset =  swn.senti_synset(synset_name)\n",
    "    if not swn_synset:\n",
    "        return None\n",
    "    elif swn_synset.pos_score() > swn_synset.neg_score() and swn_synset.pos_score() > swn_synset.obj_score():\n",
    "        return 1\n",
    "    elif swn_synset.neg_score() > swn_synset.pos_score() and swn_synset.neg_score() > swn_synset.obj_score():\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "for synset in wn.all_synsets():      \n",
    "    \n",
    "    # Nombre de synset polarité pour chaque lemme\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    neutral_count = 0\n",
    "    \n",
    "    for lemma in synset.lemma_names():\n",
    "        for syns in wn.synsets(lemma):\n",
    "            if get_polarity_type(syns.name())==1:\n",
    "                pos_count+=1\n",
    "            elif get_polarity_type(syns.name())==-1:\n",
    "                neg_count+=1\n",
    "            else:\n",
    "                neutral_count+=1\n",
    "    \n",
    "    if pos_count > neg_count and pos_count >= neutral_count: #>=neutre comme des mots qui sont plus positifs que négatifs, \n",
    "                                                                #même si elle est tout aussi neutre, peut appartenir à une liste positive (expliquer)\n",
    "        swn_positive.append(synset.lemma_names()[0])\n",
    "    elif neg_count > pos_count and neg_count >= neutral_count:\n",
    "        swn_negative.append(synset.lemma_names()[0])       \n",
    "\n",
    "swn_positive = list(set(swn_positive))\n",
    "swn_negative = list(set(swn_negative))\n",
    "            \n",
    "            \n",
    "print  (random.sample(swn_positive,5))\n",
    "\n",
    "print  (random.sample(swn_negative,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cela ressemble à un excellent ensemble de deux mots négatifs positifs, en regardant les échantillons. Mais voyons comment il se compare au jeu annoté manuellement de NLTK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexiques pour la classification\n",
    "Et si on utilisait les lexiques pour le problème principal de classification ?\n",
    "\n",
    "Créons une fonction qui calcule un score de polarité pour une phrase basée sur un lexique donné. Nous allons compter les mots positifs et négatifs qui apparaissent dans le tweet, puis retourner un +1 s'il y a plus de mots positifs, un -1 s'il y a plus de mots négatifs, et un 0 sinon.\n",
    "\n",
    "Nous comparerons ensuite les résultats des deux lexiques sur l'ensemble de développement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du lexique manuel: 55.001487652484386\n",
      "Précision du lexique automatique : 50.50877714965784\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "import math\n",
    " \n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()\n",
    "#Tous les lexiques sont convertis en ensembles pour un prétraitement plus rapide.\n",
    "manual_pos_set = set(positive_words)\n",
    "manual_neg_set = set(negative_words)\n",
    "\n",
    "syn_pos_set = set(swn_positive)\n",
    "syn_neg_set = set(swn_negative)\n",
    "\n",
    "\n",
    "#Fonction permettant de calculer le score de polarité d'une phrase en fonction de la fréquence des mots positifs ou négatifs. \n",
    "def get_polarity_score(sentence,pos_lexicon,neg_lexicon):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for word in sentence:\n",
    "        if word in pos_lexicon:\n",
    "            pos_count+=1\n",
    "        if word in neg_lexicon:\n",
    "            neg_count+=1\n",
    "    if pos_count>neg_count:\n",
    "        return 1\n",
    "    elif neg_count>pos_count:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "#Fonction permettant de calculer le score pour chaque tweet, de le comparer aux étiquettes réelles de l'ensemble de données et de calculer le score de précision. def data_polarity_accuracy(dataset,datalabels,pos_lexicon,neg_lexicon):\n",
    "    accuracy_count = 0\n",
    "    for index,tweet in enumerate(dataset):\n",
    "        if datalabels[index]==get_polarity_score([word for sentence in tweet for word in sentence],pos_lexicon,neg_lexicon):\n",
    "            accuracy_count+=1\n",
    "    return (accuracy_count/len(dataset))*100\n",
    "        \n",
    "print (\"Précision du lexique manuel: \"+str(data_polarity_accuracy(dev_data[0],dev_data[1],manual_pos_set,manual_neg_set))  )    \n",
    "print (\"Précision du lexique automatique : \"+str(data_polarity_accuracy(dev_data[0],dev_data[1],syn_pos_set,syn_neg_set))  )    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexique de la polarité avec apprentissage machine\n",
    "\n",
    "En conclusion, nous étudierons les effets de l'ajout du score de polarité comme caractéristique de notre classificateur statistique.\n",
    "\n",
    "Nous allons créer une nouvelle version de notre fonction d'extraction de fonctionnalités, pour intégrer les fonctionnalités supplémentaires et recycler notre classificateur de régression logistique pour voir s'il y a une amélioration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_feature_dicts_v2(tweets,manual,first,remove_stop_words,n): \n",
    "    feature_dicts = []\n",
    "    for tweet in tweets:\n",
    "        # Dictionnaire des fonctionnalités de build pour tweet\n",
    "        feature_dict = {}\n",
    "        if remove_stop_words:\n",
    "            for segment in tweet:\n",
    "                for token in segment:\n",
    "                    if token not in stopwords and (n<=0 or total_train_bow[token]>=n):\n",
    "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
    "        else:\n",
    "            for segment in tweet:\n",
    "                for token in segment:\n",
    "                    if n<=0 or total_train_bow[token]>=n:\n",
    "                        feature_dict[token] = feature_dict.get(token,0) + 1\n",
    "        if manual == True:\n",
    "            feature_dict['manual_polarity'] = get_polarity_score([word for sentence in tweet for word in sentence],manual_pos_set,manual_neg_set)\n",
    "        if first == True:\n",
    "            feature_dict['synset_polarity'] = get_polarity_score([word for sentence in tweet for word in sentence],syn_pos_set,syn_neg_set)\n",
    "    \n",
    "        feature_dicts.append(feature_dict)      \n",
    "    return feature_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_v2 = convert_to_feature_dicts_v2(train_tweets,True,False,True,2)\n",
    "\n",
    "training_data_v2 = vectorizer.fit_transform(training_set_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision de la Logistic Regression V2 (avec les scores de polarité) : 0.6809878012496281\n"
     ]
    }
   ],
   "source": [
    "dev_set_v2 = convert_to_feature_dicts_v2(dev_data[0],True,False,False,0)\n",
    "\n",
    "development_data_v2 = vectorizer.transform(dev_set_v2)\n",
    "\n",
    "log_clf_v2 = LogisticRegression(C=0.012,solver='lbfgs',multi_class='multinomial')\n",
    "\n",
    "log_clf_v2.fit(training_data_v2,train_data[1])\n",
    "\n",
    "log_predictions_v2 = log_clf_v2.predict(development_data_v2)\n",
    "\n",
    "print (\"Précision de la Logistic Regression V2 (avec les scores de polarité) : \" + str(accuracy_score(dev_data[1],log_predictions_v2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que minime, il y a eu une certaine amélioration dans le classificateur en intégrant les données de polarité.\n",
    "\n",
    "Ceci conclut notre projet de construction d'un classificateur de polarité de base à 2 voies pour les tweets.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
